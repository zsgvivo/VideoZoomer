python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=grpo \
    data.train_files="/mnt/bn/tiktok-mm-4/aiic/users/dingyang/data/ytb_json_files/processed/mc_pass@8_0_mc_to_oe_train.yaml" \
    data.val_files="/mnt/bn/tiktok-mm-4/aiic/users/dingyang/data/ytb_json_files/processed/mc_pass@8_0_mc_to_oe_val.yaml" \
    data.train_batch_size=128 \
    data.max_prompt_length=16384 \
    data.max_response_length=8192 \
    data.image_key=videos \
    data.video_fps=0.2 \
    data.max_pixels=100352 \
    data.min_pixels=25088 \
    data.storage_system=tos \
    reward_model.reward_manager=naive_multithreads \
    reward_model.val_reward_manager=naive_multithreads \
    'data.system_prompt="You are a helpful assistant. You will receive a low-frame-rate video and related questions. You can analyze the video content to answer the question and trigger high-frame-rate inspections when finer temporal resolution is needed. When you detect ambiguous motion/objects that require closer inspection, wrap your request in <video_zoom></video_zoom> tags and provide the exact time segment and target frame rate in JSON format: <video_zoom> {\"segment\": [start_sec, end_sec], \"fps\": n} </video_zoom>, it will return the video clip at the target fps to help you better answer the question. Note that the total frames num of the request clip cannot exceed 16 (e.g., (end_sec - start_sec) * fps â‰¤ 16) and DO NOT include <answer> tags in this round. \n Example usage: <video_zoom> {\"segment\": [4.0, 6.0], \"fps\": 2} </video_zoom>.\n Output the thinking process within <think> </think> tags, once you confirm your final answer, place the final answer in \\boxed{{}} inside <answer> and </answer>."' \
    actor_rollout_ref.model.path=/mnt/bn/tiktok-mm-4/aiic/users/dingyang/models/Qwen2.5-VL-7B-Instruct \
    actor_rollout_ref.actor.optim.lr=1e-6 \
    actor_rollout_ref.actor.optim.lr_scheduler=constant \
    actor_rollout_ref.actor.clip_ratio=0.2 \
    actor_rollout_ref.model.use_remove_padding=True \
    actor_rollout_ref.actor.ppo_mini_batch_size=32 \
    actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=2 \
    actor_rollout_ref.actor.use_kl_loss=True \
    actor_rollout_ref.actor.kl_loss_coef=0.001 \
    actor_rollout_ref.actor.entropy_coeff=0.001 \
    actor_rollout_ref.actor.kl_loss_type=low_var_kl \
    actor_rollout_ref.model.enable_gradient_checkpointing=True \
    actor_rollout_ref.actor.fsdp_config.param_offload=False \
    actor_rollout_ref.actor.fsdp_config.optimizer_offload=False \
    actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=4 \
    actor_rollout_ref.rollout.tensor_model_parallel_size=2 \
    actor_rollout_ref.rollout.max_num_batched_tokens=65536 \
    actor_rollout_ref.rollout.name=vllm_video_multiturn \
    actor_rollout_ref.rollout.gpu_memory_utilization=0.6 \
    actor_rollout_ref.rollout.enable_chunked_prefill=False \
    actor_rollout_ref.rollout.enforce_eager=False \
    actor_rollout_ref.rollout.free_cache_engine=False \
    actor_rollout_ref.rollout.n=16 \
    actor_rollout_ref.rollout.max_total_response_length=65536 \
    actor_rollout_ref.rollout.max_model_len=65536 \
    actor_rollout_ref.rollout.max_generation_round=2 \
    'actor_rollout_ref.rollout.limit_mm_per_prompt={'image': 50}' \
    actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4 \
    actor_rollout_ref.ref.fsdp_config.param_offload=True \
    algorithm.kl_ctrl.kl_coef=0.0 \
    trainer.critic_warmup=0 \
    trainer.logger=['console','wandb'] \
    trainer.project_name='dingyang_verl_grpo_video_debug' \
    trainer.experiment_name='qwen2_5_vl_7b_multiturn_video_youtube_mc_to_oe_pass@8_0_clip0.2_res8k_rollout16_0.1formatv2_rejsample_penalty-0.5' \
    trainer.log_training_rollouts_freq=5 \
    trainer.train_generations_to_log_to_wandb=256 \
    trainer.val_generations_to_log_to_wandb=128 \
    trainer.tool_call_generations_to_log_to_wandb=10 \
    trainer.n_gpus_per_node=8 \
    trainer.nnodes=1 \
    trainer.save_freq=20 \
    trainer.rejection_sample=True \
    trainer.test_freq=5 \
    trainer.total_epochs=5 \
    trainer.resume_mode=disable \
    'trainer.rollout_data_dir=logs/train' \
    'trainer.validation_data_dir=logs/val' \
    reward_model.log_rewards_separately=True \
    reward_model.acc_reward_weight=0.9 \
    reward_model.format_reward_weight=0.1 \
    reward_model.tool_call_penalty=-0.5 \
    reward_model.gpt_extract_answer=True \
    trainer.reflection_keywords=['wait,recheck,alternatively,retry,however,rethink,re-evaluate,hmm,re-check,double-check'] \
    'trainer.default_local_dir=/mnt/hdfs/yangding/checkpoints/${trainer.project_name}/${trainer.experiment_name}'